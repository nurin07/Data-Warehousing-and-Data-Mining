{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMr2kuqyAYa5xe/+HVn2341",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nurin07/Data-Warehousing-and-Data-Mining/blob/main/Lab_II.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Ez2e3daLCGPN",
        "outputId": "355c00ec-7f80-41f2-a663-6a21095249c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequent Itemsets:\n",
            "     support        itemsets\n",
            "0  0.352941  (cricket ball)\n",
            "1  0.392157   (cricket bat)\n",
            "2  0.431373      (football)\n",
            "3  0.352941        (gloves)\n",
            "4  0.254902     (ice cream)\n",
            "5  0.411765         (juice)\n",
            "6  0.274510  (water bottle)\n",
            "\n",
            "No association rules found with confidence ≥ 0.7\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "\n",
        "file_path = '/content/sports.txt'\n",
        "transactions = []\n",
        "\n",
        "with open(file_path, 'r') as file:\n",
        "    next(file)\n",
        "    for line in file:\n",
        "        parts = line.strip().split(',')\n",
        "        transactions.append(parts[1:])\n",
        "\n",
        "te = TransactionEncoder()\n",
        "te_ary = te.fit(transactions).transform(transactions)\n",
        "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "\n",
        "frequent_itemsets = apriori(df, min_support=0.15, use_colnames=True)\n",
        "\n",
        "min_confidence = 0.7\n",
        "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\n",
        "\n",
        "print(\"Frequent Itemsets:\\n\", frequent_itemsets)\n",
        "if not rules.empty:\n",
        "    print(\"\\nAssociation Rules:\\n\", rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n",
        "else:\n",
        "    print(\"\\nNo association rules found with confidence ≥\", min_confidence)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from itertools import combinations\n",
        "from collections import defaultdict\n",
        "\n",
        "file_path = '/content/sports.txt'\n",
        "transactions = []\n",
        "\n",
        "with open(file_path, 'r') as file:\n",
        "    next(file)\n",
        "    for line in file:\n",
        "        parts = line.strip().split(',')\n",
        "        transactions.append([item.strip() for item in parts[1:] if item.strip()])\n",
        "\n",
        "total_tx = len(transactions)\n",
        "min_support = 0.15\n",
        "min_confidence = 0.7\n",
        "\n",
        "def get_support(itemset, transactions):\n",
        "    count = sum(1 for tx in transactions if itemset.issubset(set(tx)))\n",
        "    return count / total_tx\n",
        "\n",
        "def apriori(transactions, min_support):\n",
        "    item_counts = defaultdict(int)\n",
        "\n",
        "    for tx in transactions:\n",
        "        for item in tx:\n",
        "            item_counts[frozenset([item])] += 1\n",
        "\n",
        "    frequent_itemsets = {item: count for item, count in item_counts.items() if count / total_tx >= min_support}\n",
        "    all_frequent = frequent_itemsets.copy()\n",
        "    current_freq = list(frequent_itemsets.keys())\n",
        "    k = 2\n",
        "\n",
        "    while current_freq:\n",
        "        candidates = set()\n",
        "        for i in range(len(current_freq)):\n",
        "            for j in range(i + 1, len(current_freq)):\n",
        "                union = current_freq[i] | current_freq[j]\n",
        "                if len(union) == k:\n",
        "                    candidates.add(union)\n",
        "\n",
        "        candidate_counts = defaultdict(int)\n",
        "        for tx in transactions:\n",
        "            tx_set = set(tx)\n",
        "            for candidate in candidates:\n",
        "                if candidate.issubset(tx_set):\n",
        "                    candidate_counts[candidate] += 1\n",
        "\n",
        "        current_freq = [item for item in candidate_counts if candidate_counts[item] / total_tx >= min_support]\n",
        "        all_frequent.update({item: candidate_counts[item] for item in current_freq})\n",
        "        k += 1\n",
        "\n",
        "    return all_frequent\n",
        "\n",
        "def generate_rules(frequent_itemsets, transactions, min_confidence):\n",
        "    rules = []\n",
        "    for itemset in frequent_itemsets:\n",
        "        if len(itemset) < 2:\n",
        "            continue\n",
        "        support_itemset = frequent_itemsets[itemset] / total_tx\n",
        "        for i in range(1, len(itemset)):\n",
        "            for antecedent in combinations(itemset, i):\n",
        "                antecedent = frozenset(antecedent)\n",
        "                consequent = itemset - antecedent\n",
        "                support_ante = get_support(antecedent, transactions)\n",
        "                support_cons = get_support(consequent, transactions)\n",
        "                confidence = support_itemset / support_ante\n",
        "                lift = confidence / support_cons\n",
        "                if confidence >= min_confidence:\n",
        "                    rules.append({\n",
        "                        'antecedents': set(antecedent),\n",
        "                        'consequents': set(consequent),\n",
        "                        'support': round(support_itemset, 2),\n",
        "                        'confidence': round(confidence, 2),\n",
        "                        'lift': round(lift, 2)\n",
        "                    })\n",
        "    return rules\n",
        "\n",
        "frequent_itemsets_raw = apriori(transactions, min_support)\n",
        "rules = generate_rules(frequent_itemsets_raw, transactions, min_confidence)\n",
        "\n",
        "frequent_itemsets_df = pd.DataFrame([{\n",
        "    'itemsets': set(item),\n",
        "    'support': round(count / total_tx, 2)\n",
        "} for item, count in frequent_itemsets_raw.items()])\n",
        "\n",
        "rules_df = pd.DataFrame(rules)\n",
        "\n",
        "print(\"Frequent Itemsets:\\n\", frequent_itemsets_df)\n",
        "\n",
        "if not rules_df.empty:\n",
        "    print(\"\\nAssociation Rules:\\n\", rules_df[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n",
        "else:\n",
        "    print(\"\\nNo association rules found with confidence ≥\", min_confidence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyM0CRTyCJjn",
        "outputId": "bbd06826-36ae-4a81-a1de-766ca001f267"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequent Itemsets:\n",
            "          itemsets  support\n",
            "0      {football}     0.43\n",
            "1  {cricket ball}     0.35\n",
            "2        {gloves}     0.35\n",
            "3   {cricket bat}     0.39\n",
            "4         {juice}     0.41\n",
            "5  {water bottle}     0.27\n",
            "6     {ice cream}     0.25\n",
            "\n",
            "No association rules found with confidence ≥ 0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from itertools import combinations\n",
        "from collections import defaultdict\n",
        "\n",
        "file_path = 'space.txt'\n",
        "transactions = []\n",
        "\n",
        "with open(file_path, 'r') as file:\n",
        "    next(file)\n",
        "    for line in file:\n",
        "        parts = line.strip().split(',')\n",
        "        transactions.append([item.strip() for item in parts[1:] if item.strip()])\n",
        "\n",
        "total_tx = len(transactions)\n",
        "min_support = 0.15\n",
        "min_confidence = 0.7\n",
        "\n",
        "def get_support(itemset, transactions):\n",
        "    count = sum(1 for tx in transactions if itemset.issubset(set(tx)))\n",
        "    return count / total_tx\n",
        "\n",
        "def apriori(transactions, min_support):\n",
        "    item_counts = defaultdict(int)\n",
        "\n",
        "    for tx in transactions:\n",
        "        for item in tx:\n",
        "            item_counts[frozenset([item])] += 1\n",
        "\n",
        "    frequent_itemsets = {item: count for item, count in item_counts.items() if count / total_tx >= min_support}\n",
        "    all_frequent = frequent_itemsets.copy()\n",
        "    current_freq = list(frequent_itemsets.keys())\n",
        "    k = 2\n",
        "\n",
        "    while current_freq:\n",
        "        candidates = set()\n",
        "        for i in range(len(current_freq)):\n",
        "            for j in range(i + 1, len(current_freq)):\n",
        "                union = current_freq[i] | current_freq[j]\n",
        "                if len(union) == k:\n",
        "                    candidates.add(union)\n",
        "\n",
        "        candidate_counts = defaultdict(int)\n",
        "        for tx in transactions:\n",
        "            tx_set = set(tx)\n",
        "            for candidate in candidates:\n",
        "                if candidate.issubset(tx_set):\n",
        "                    candidate_counts[candidate] += 1\n",
        "\n",
        "        current_freq = [item for item in candidate_counts if candidate_counts[item] / total_tx >= min_support]\n",
        "        all_frequent.update({item: candidate_counts[item] for item in current_freq})\n",
        "        k += 1\n",
        "\n",
        "    return all_frequent\n",
        "\n",
        "def generate_rules(frequent_itemsets, transactions, min_confidence):\n",
        "    rules = []\n",
        "    for itemset in frequent_itemsets:\n",
        "        if len(itemset) < 2:\n",
        "            continue\n",
        "        support_itemset = frequent_itemsets[itemset] / total_tx\n",
        "        for i in range(1, len(itemset)):\n",
        "            for antecedent in combinations(itemset, i):\n",
        "                antecedent = frozenset(antecedent)\n",
        "                consequent = itemset - antecedent\n",
        "                support_ante = get_support(antecedent, transactions)\n",
        "                support_cons = get_support(consequent, transactions)\n",
        "                confidence = support_itemset / support_ante\n",
        "                lift = confidence / support_cons\n",
        "                if confidence >= min_confidence:\n",
        "                    rules.append({\n",
        "                        'antecedents': set(antecedent),\n",
        "                        'consequents': set(consequent),\n",
        "                        'support': round(support_itemset, 2),\n",
        "                        'confidence': round(confidence, 2),\n",
        "                        'lift': round(lift, 2)\n",
        "                    })\n",
        "    return rules\n",
        "\n",
        "frequent_itemsets_raw = apriori(transactions, min_support)\n",
        "rules = generate_rules(frequent_itemsets_raw, transactions, min_confidence)\n",
        "\n",
        "frequent_itemsets_df = pd.DataFrame([{\n",
        "    'itemsets': set(item),\n",
        "    'support': round(count / total_tx, 2)\n",
        "} for item, count in frequent_itemsets_raw.items()])\n",
        "\n",
        "rules_df = pd.DataFrame(rules)\n",
        "\n",
        "print(\"Frequent Itemsets:\\n\", frequent_itemsets_df)\n",
        "\n",
        "if not rules_df.empty:\n",
        "    print(\"\\nAssociation Rules:\\n\", rules_df[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n",
        "else:\n",
        "    print(\"\\nNo association rules found with confidence ≥\", min_confidence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FX8EGsDiCLwp",
        "outputId": "4a59a177-a149-4ae4-e6bc-b8b34669cc5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequent Itemsets:\n",
            "                      itemsets  support\n",
            "0               {Robotic Arm}     0.33\n",
            "1              {Food Packets}     0.39\n",
            "2              {Sleeping Bag}     0.31\n",
            "3                 {Treadmill}     0.27\n",
            "4                {Space Suit}     0.31\n",
            "5                {3D Printer}     0.27\n",
            "6  {Carbon Dioxide Scrubbers}     0.24\n",
            "\n",
            "No association rules found with confidence ≥ 0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "file_path = 'sports.txt'\n",
        "transactions = []\n",
        "\n",
        "with open(file_path, 'r') as file:\n",
        "    next(file)\n",
        "    for line in file:\n",
        "        parts = line.strip().split(',')\n",
        "        transactions.append([item.strip() for item in parts[1:] if item.strip()])\n",
        "\n",
        "total_tx = len(transactions)\n",
        "min_support = 0.15\n",
        "min_conf = 0.7\n",
        "\n",
        "item_counts = Counter()\n",
        "for tx in transactions:\n",
        "    for item in tx:\n",
        "        item_counts[item] += 1\n",
        "\n",
        "item_counts = {item: count for item, count in item_counts.items() if count / total_tx >= min_support}\n",
        "items_ordered = sorted(item_counts, key=lambda x: (-item_counts[x], x))\n",
        "\n",
        "def reorder_transaction(tx):\n",
        "    return [item for item in items_ordered if item in tx]\n",
        "\n",
        "ordered_tx = [reorder_transaction(tx) for tx in transactions]\n",
        "\n",
        "class FPNode:\n",
        "    def __init__(self, item, parent):\n",
        "        self.item = item\n",
        "        self.count = 1\n",
        "        self.parent = parent\n",
        "        self.children = {}\n",
        "        self.link = None\n",
        "\n",
        "class FPTree:\n",
        "    def __init__(self, transactions):\n",
        "        self.root = FPNode(None, None)\n",
        "        self.header_table = {}\n",
        "        for tx in transactions:\n",
        "            self.insert(tx)\n",
        "\n",
        "    def insert(self, tx):\n",
        "        node = self.root\n",
        "        for item in tx:\n",
        "            if item not in node.children:\n",
        "                node.children[item] = FPNode(item, node)\n",
        "                if item not in self.header_table:\n",
        "                    self.header_table[item] = node.children[item]\n",
        "                else:\n",
        "                    curr = self.header_table[item]\n",
        "                    while curr.link is not None:\n",
        "                        curr = curr.link\n",
        "                    curr.link = node.children[item]\n",
        "            else:\n",
        "                node.children[item].count += 1\n",
        "            node = node.children[item]\n",
        "\n",
        "def mine_tree(tree, suffix, freq_items):\n",
        "    items = sorted(tree.header_table.items(), key=lambda x: item_counts[x[0]])\n",
        "    for item, node in items:\n",
        "        new_suffix = suffix + [item]\n",
        "        support = 0\n",
        "        conditional_patterns = []\n",
        "\n",
        "        while node:\n",
        "            support += node.count\n",
        "            path = []\n",
        "            parent = node.parent\n",
        "            while parent and parent.item:\n",
        "                path.append(parent.item)\n",
        "                parent = parent.parent\n",
        "            if path:\n",
        "                conditional_patterns.append((path[::-1], node.count))\n",
        "            node = node.link\n",
        "\n",
        "        if support / total_tx >= min_support:\n",
        "            freq_items[frozenset(new_suffix)] = support\n",
        "\n",
        "            cond_tx = []\n",
        "            for path, count in conditional_patterns:\n",
        "                cond_tx.extend([path] * count)\n",
        "            if cond_tx:\n",
        "                cond_tree = FPTree(cond_tx)\n",
        "                mine_tree(cond_tree, new_suffix, freq_items)\n",
        "\n",
        "fp_tree = FPTree(ordered_tx)\n",
        "frequent_itemsets = {}\n",
        "mine_tree(fp_tree, [], frequent_itemsets)\n",
        "\n",
        "frequent_itemsets_df = pd.DataFrame([{\n",
        "    'itemsets': set(item),\n",
        "    'support': round(count / total_tx, 2)\n",
        "} for item, count in frequent_itemsets.items()])\n",
        "\n",
        "def get_support(itemset):\n",
        "    return sum(1 for tx in transactions if itemset.issubset(set(tx))) / total_tx\n",
        "\n",
        "rules = []\n",
        "for itemset in frequent_itemsets:\n",
        "    if len(itemset) < 2:\n",
        "        continue\n",
        "    support = frequent_itemsets[itemset] / total_tx\n",
        "    for i in range(1, len(itemset)):\n",
        "        from itertools import combinations\n",
        "        for antecedent in combinations(itemset, i):\n",
        "            antecedent = frozenset(antecedent)\n",
        "            consequent = itemset - antecedent\n",
        "            support_ante = get_support(antecedent)\n",
        "            support_cons = get_support(consequent)\n",
        "            confidence = support / support_ante\n",
        "            lift = confidence / support_cons\n",
        "            if confidence >= min_conf:\n",
        "                rules.append({\n",
        "                    'antecedents': set(antecedent),\n",
        "                    'consequents': set(consequent),\n",
        "                    'support': round(support, 2),\n",
        "                    'confidence': round(confidence, 2),\n",
        "                    'lift': round(lift, 2)\n",
        "                })\n",
        "\n",
        "rules_df = pd.DataFrame(rules)\n",
        "\n",
        "print(\"Frequent Itemsets:\\n\", frequent_itemsets_df)\n",
        "if not rules_df.empty:\n",
        "    print(\"\\nAssociation Rules:\\n\", rules_df[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n",
        "else:\n",
        "    print(\"\\nNo rules meet the minimum confidence threshold.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEZ_BS_FCN0u",
        "outputId": "82e5f2ac-6b15-49cc-c3bc-6acb067240ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequent Itemsets:\n",
            "          itemsets  support\n",
            "0     {ice cream}     0.25\n",
            "1  {water bottle}     0.27\n",
            "2  {cricket ball}     0.35\n",
            "3        {gloves}     0.35\n",
            "4   {cricket bat}     0.39\n",
            "5         {juice}     0.41\n",
            "6      {football}     0.43\n",
            "\n",
            "No rules meet the minimum confidence threshold.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "file_path = 'space.txt'\n",
        "transactions = []\n",
        "\n",
        "with open(file_path, 'r') as file:\n",
        "    next(file)\n",
        "    for line in file:\n",
        "        parts = line.strip().split(',')\n",
        "        transactions.append([item.strip() for item in parts[1:] if item.strip()])\n",
        "\n",
        "total_tx = len(transactions)\n",
        "min_support = 0.15\n",
        "min_conf = 0.7\n",
        "\n",
        "item_counts = Counter()\n",
        "for tx in transactions:\n",
        "    for item in tx:\n",
        "        item_counts[item] += 1\n",
        "\n",
        "item_counts = {item: count for item, count in item_counts.items() if count / total_tx >= min_support}\n",
        "items_ordered = sorted(item_counts, key=lambda x: (-item_counts[x], x))\n",
        "\n",
        "def reorder_transaction(tx):\n",
        "    return [item for item in items_ordered if item in tx]\n",
        "\n",
        "ordered_tx = [reorder_transaction(tx) for tx in transactions]\n",
        "\n",
        "class FPNode:\n",
        "    def __init__(self, item, parent):\n",
        "        self.item = item\n",
        "        self.count = 1\n",
        "        self.parent = parent\n",
        "        self.children = {}\n",
        "        self.link = None\n",
        "\n",
        "class FPTree:\n",
        "    def __init__(self, transactions):\n",
        "        self.root = FPNode(None, None)\n",
        "        self.header_table = {}\n",
        "        for tx in transactions:\n",
        "            self.insert(tx)\n",
        "\n",
        "    def insert(self, tx):\n",
        "        node = self.root\n",
        "        for item in tx:\n",
        "            if item not in node.children:\n",
        "                node.children[item] = FPNode(item, node)\n",
        "                if item not in self.header_table:\n",
        "                    self.header_table[item] = node.children[item]\n",
        "                else:\n",
        "                    curr = self.header_table[item]\n",
        "                    while curr.link is not None:\n",
        "                        curr = curr.link\n",
        "                    curr.link = node.children[item]\n",
        "            else:\n",
        "                node.children[item].count += 1\n",
        "            node = node.children[item]\n",
        "\n",
        "def mine_tree(tree, suffix, freq_items):\n",
        "    items = sorted(tree.header_table.items(), key=lambda x: item_counts[x[0]])\n",
        "    for item, node in items:\n",
        "        new_suffix = suffix + [item]\n",
        "        support = 0\n",
        "        conditional_patterns = []\n",
        "\n",
        "        while node:\n",
        "            support += node.count\n",
        "            path = []\n",
        "            parent = node.parent\n",
        "            while parent and parent.item:\n",
        "                path.append(parent.item)\n",
        "                parent = parent.parent\n",
        "            if path:\n",
        "                conditional_patterns.append((path[::-1], node.count))\n",
        "            node = node.link\n",
        "\n",
        "        if support / total_tx >= min_support:\n",
        "            freq_items[frozenset(new_suffix)] = support\n",
        "\n",
        "            cond_tx = []\n",
        "            for path, count in conditional_patterns:\n",
        "                cond_tx.extend([path] * count)\n",
        "            if cond_tx:\n",
        "                cond_tree = FPTree(cond_tx)\n",
        "                mine_tree(cond_tree, new_suffix, freq_items)\n",
        "\n",
        "fp_tree = FPTree(ordered_tx)\n",
        "frequent_itemsets = {}\n",
        "mine_tree(fp_tree, [], frequent_itemsets)\n",
        "\n",
        "frequent_itemsets_df = pd.DataFrame([{\n",
        "    'itemsets': set(item),\n",
        "    'support': round(count / total_tx, 2)\n",
        "} for item, count in frequent_itemsets.items()])\n",
        "\n",
        "def get_support(itemset):\n",
        "    return sum(1 for tx in transactions if itemset.issubset(set(tx))) / total_tx\n",
        "\n",
        "rules = []\n",
        "for itemset in frequent_itemsets:\n",
        "    if len(itemset) < 2:\n",
        "        continue\n",
        "    support = frequent_itemsets[itemset] / total_tx\n",
        "    for i in range(1, len(itemset)):\n",
        "        from itertools import combinations\n",
        "        for antecedent in combinations(itemset, i):\n",
        "            antecedent = frozenset(antecedent)\n",
        "            consequent = itemset - antecedent\n",
        "            support_ante = get_support(antecedent)\n",
        "            support_cons = get_support(consequent)\n",
        "            confidence = support / support_ante\n",
        "            lift = confidence / support_cons\n",
        "            if confidence >= min_conf:\n",
        "                rules.append({\n",
        "                    'antecedents': set(antecedent),\n",
        "                    'consequents': set(consequent),\n",
        "                    'support': round(support, 2),\n",
        "                    'confidence': round(confidence, 2),\n",
        "                    'lift': round(lift, 2)\n",
        "                })\n",
        "\n",
        "rules_df = pd.DataFrame(rules)\n",
        "\n",
        "print(\"Frequent Itemsets:\\n\", frequent_itemsets_df)\n",
        "if not rules_df.empty:\n",
        "    print(\"\\nAssociation Rules:\\n\", rules_df[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n",
        "else:\n",
        "    print(\"\\nNo rules meet the minimum confidence threshold.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciM42VwICPzA",
        "outputId": "430c9e50-4687-454c-edcd-c8d1ad15740a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequent Itemsets:\n",
            "                      itemsets  support\n",
            "0  {Carbon Dioxide Scrubbers}     0.24\n",
            "1                 {Treadmill}     0.27\n",
            "2                {3D Printer}     0.27\n",
            "3              {Sleeping Bag}     0.31\n",
            "4                {Space Suit}     0.31\n",
            "5               {Robotic Arm}     0.33\n",
            "6              {Food Packets}     0.39\n",
            "\n",
            "No rules meet the minimum confidence threshold.\n"
          ]
        }
      ]
    }
  ]
}